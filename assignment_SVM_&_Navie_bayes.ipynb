{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "THEORY QUESTION"
      ],
      "metadata": {
        "id": "6LotO_LtCkOS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q-1  What is Information Gain, and how is it used in Decision Trees?\n",
        "\n",
        "  ANS--Information Gain  is a measure used in Decision Trees (e.g., ID3, C4.5) to decide which feature to split on at each step of the tree-building process.\n",
        "\n",
        "   how information gain is used in decisions trees:\n",
        "\n",
        "   1. feature selection at each node\n",
        "    \n",
        "    * Compute IG for each candidate feature.\n",
        "\n",
        "    * Choose the feature that gives the highest Information Gain.\n",
        "\n",
        "    * This feature maximally reduces uncertainty about the target.\n",
        "\n",
        "  2. create more \"pure\" child nodes\n",
        "   \n",
        "    * More homogeneous\n",
        "\n",
        "    * Closer to a single class\n",
        "\n",
        "    * Easier to classify\n",
        "\n",
        "  3. Helps control tree structure\n",
        "   \n",
        "    * High IG → good split\n",
        "\n",
        "    * Low IG → poor split\n",
        "This affects the depth and accuracy of the final tree."
      ],
      "metadata": {
        "id": "DNWj9kUmCy2-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q- 2  What is the difference between Gini Impurity and Entropy?\n",
        "\n",
        "  ANS--\n",
        "   \n",
        "   | Aspect                | Gini Impurity                 | Entropy                |\n",
        "| --------------------- | ----------------------------- | ---------------------- |\n",
        "| Used in               | CART                          | ID3, C4.5              |\n",
        "| Formula               | (1 - \\sum p_i^2)              | (-\\sum p_i \\log_2 p_i) |\n",
        "| Interpretation        | Misclassification probability | Measure of uncertainty |\n",
        "| Computation           | Faster                        | Slightly slower (log)  |\n",
        "| Splitting tendency    | Favors pure class splits      | Favors balanced splits |\n",
        "| Output range (binary) | 0 → 0.5                       | 0 → 1                  |\n"
      ],
      "metadata": {
        "id": "Z7Kv6_ISEzqF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q-3 :What is Pre-Pruning in Decision Trees?\n",
        "\n",
        "  ANS-- Pre-pruning prevents the decision tree from splitting a node if the split does not provide sufficient improvement according to some criterion.\n",
        "Instead of growing a full tree and pruning afterward, the algorithm blocks certain splits during training.\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "r8j2NfY9Fmmh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q-4 Write a Python program to train a Decision Tree Classifier using Gini\n",
        "Impurity as the criterion and print the feature importances (practical)."
      ],
      "metadata": {
        "id": "fxG6CSnfGMDs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# 1. Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# 2. Split dataset into train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Train Decision Tree using Gini impurity\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# 4. Print feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for name, importance in zip(feature_names, clf.feature_importances_):\n",
        "    print(f\"{name}: {importance:.4f}\")\n",
        "\n",
        "# 5. Optional: Accuracy\n",
        "accuracy = clf.score(X_test, y_test)\n",
        "print(\"\\nModel Accuracy:\", accuracy)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFRg6UoAGxlB",
        "outputId": "b7d7d231-c8d2-4c39-ed72-2926f486a436"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0191\n",
            "petal length (cm): 0.8933\n",
            "petal width (cm): 0.0876\n",
            "\n",
            "Model Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q-5  What is a Support Vector Machine (SVM)?\n",
        "\n",
        "  ANS-- A Support Vector Machine finds the best separating boundary (hyperplane) between classes by maximizing the margin—the distance between the boundary and the closest data points (called support vectors)."
      ],
      "metadata": {
        "id": "n6dPVm5GHBwY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q-6 What is the Kernel Trick in SVM?\n",
        "\n",
        "  ANS--The Kernel Trick is a technique used in Support Vector Machines (SVMs) that allows the model to learn non-linear decision boundaries without explicitly transforming the data into higher dimensions."
      ],
      "metadata": {
        "id": "UpxvAmqfHdV2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q-7  Write a Python program to train two SVM classifiers with Linear and RBF\n",
        "kernels on the Wine dataset, then compare their accuracies."
      ],
      "metadata": {
        "id": "iwRjdTlIH3Du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# 2. Standardize features (important for SVM)\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# 3. Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# 4. Train SVM with Linear Kernel\n",
        "svm_linear = SVC(kernel='linear')\n",
        "svm_linear.fit(X_train, y_train)\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "acc_linear = accuracy_score(y_test, y_pred_linear)\n",
        "\n",
        "# 5. Train SVM with RBF Kernel\n",
        "svm_rbf = SVC(kernel='rbf')\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "acc_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "# 6. Print results\n",
        "print(\"Linear Kernel Accuracy:\", acc_linear)\n",
        "print(\"RBF Kernel Accuracy:\", acc_rbf)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIzR9fy6IIr-",
        "outputId": "2f8b8b8b-520a-4670-918f-f04490fb997f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Kernel Accuracy: 0.9814814814814815\n",
            "RBF Kernel Accuracy: 0.9814814814814815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q-8 What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "\n",
        "  ANS--A Naïve Bayes classifier is a probabilistic machine learning model based on Bayes’ Theorem, commonly used for classification tasks such as spam detection, text classification, sentiment analysis, and more.\n",
        "\n",
        "  Why is it called navie:\n",
        "   \n",
        "   It is called naïve because it makes a strong and unrealistic assumption\n"
      ],
      "metadata": {
        "id": "hcb53TSNIdx5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q-9  Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve\n",
        "Bayes, and Bernoulli Naïve Bayes\n",
        "\n",
        "  ans-- 1.Gaussian Naïve Bayes\n",
        "\n",
        "   Continuous features\n",
        " Data that follows a normal (Gaussian) distribution\n",
        "\n",
        "       2. Multinomial Naïve Bayes\n",
        "\n",
        "       Count-based features\n",
        "       Text classification (word frequencies or counts)\n",
        "\n",
        "       3. Bernoulli Naïve Bayes\n",
        "         \n",
        "         Binary (0/1) features\n",
        "         Whether a word appears or not (not how many times)\n",
        "\n",
        "       "
      ],
      "metadata": {
        "id": "OB_KwlgsJa1a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q-10 Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer\n",
        "dataset and evaluate accuracy."
      ],
      "metadata": {
        "id": "IYE_DBmWKVln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# 1. Load the dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# 2. Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Train Gaussian Naïve Bayes classifier\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# 4. Make predictions\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# 5. Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy of Gaussian Naïve Bayes classifier:\", accuracy)\n",
        "\n",
        "# Optional: Detailed classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FrstQBdKe0U",
        "outputId": "c43dbb27-ec45-4619-c41f-ca0d9fccf5c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Gaussian Naïve Bayes classifier: 0.9415204678362573\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   malignant       0.93      0.90      0.92        63\n",
            "      benign       0.95      0.96      0.95       108\n",
            "\n",
            "    accuracy                           0.94       171\n",
            "   macro avg       0.94      0.93      0.94       171\n",
            "weighted avg       0.94      0.94      0.94       171\n",
            "\n"
          ]
        }
      ]
    }
  ]
}